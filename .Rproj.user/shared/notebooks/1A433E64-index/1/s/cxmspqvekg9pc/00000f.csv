"0","file_info <- data.frame(filepath = ""data/wos.xlsx"")"
"0","df <- TextAnalysisR::process_files(dataset_choice = ""Upload Your File"","
"0","                                       file_info = file_info)"
"0",""
"0","df <- df %>%"
"0","  filter("
"0","    PY >= 2010 &"
"0","    ("
"0","      str_detect(AB, regex(""learn.* disab.*"", ignore_case = TRUE)) |"
"0","      str_detect(DE, regex(""learn.* disab.*"", ignore_case = TRUE)) |"
"0","      str_detect(TI, regex(""learn.* disab.*"", ignore_case = TRUE))"
"0","    )"
"0","  )"
"0",""
"0","united_tbl <- TextAnalysisR::unite_text_cols(df, listed_vars = c(""AB"", ""DE"", ""TI""))"
"0",""
"0","tokens <- TextAnalysisR::preprocess_texts(united_tbl,"
"0","                                          text_field = ""united_texts"","
"0","                                          min_char = 2,"
"0","                                          remove_punct = TRUE,"
"0","                                          remove_symbols = TRUE,"
"0","                                          remove_numbers = TRUE,"
"0","                                          remove_url = TRUE,"
"0","                                          remove_separators = TRUE,"
"0","                                          split_hyphens = TRUE,"
"0","                                          split_tags = TRUE,"
"0","                                          include_docvars = TRUE,"
"0","                                          keep_acronyms = FALSE,"
"0","                                          padding = FALSE,"
"0","                                          verbose = FALSE)"
"2","G2;H2;Warningh: keep_acronyms argument is not used.g
"
"0","custom_dict <- quanteda::dictionary(list(custom = c(""learning disabilities"", ""single-case"", ""single-subject"", ""functional relation"", ""visual analysis"")))"
"0",""
"0","toks_compound <- quanteda::tokens_compound("
"0","  tokens,"
"0","  pattern = custom_dict,"
"0","  concatenator = ""_"","
"0","  valuetype = ""glob"","
"0","  window = 0,"
"0","  case_insensitive = TRUE,"
"0","  join = TRUE,"
"0","  keep_unigrams = FALSE,"
"0","  verbose = TRUE"
"0",")"
"2","G3;tokens_compound() changed from 29,088 tokens (148 documents) to 28,975 tokens (148 documents)
g"
"0","dfm_object_init <- quanteda::dfm(toks_compound)"
"0",""
"0","stopwords <- stopwords::stopwords(""en"", source = ""snowball"")"
"0",""
"0","toks_removed <- quanteda::tokens_remove(toks_compound, pattern = stopwords, verbose = FALSE)"
"0",""
"0","dfm_init <- quanteda::dfm(toks_removed)"
"0",""
"0","common_words <- c(""study"", ""students"", ""research"", ""results"")"
"0",""
"0","toks_removed_common <- quanteda::tokens_remove(toks_removed, pattern = common_words, verbose = FALSE)"
"0",""
"0","dfm_init_updated <- quanteda::dfm(toks_removed_common)"
"0",""
"0","TextAnalysisR::plot_word_frequency(dfm_init_updated, n = 20)"
