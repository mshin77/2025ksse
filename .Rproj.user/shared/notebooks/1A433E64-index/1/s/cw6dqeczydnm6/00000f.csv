"0","df_2010_later <- df %>%"
"0","  filter("
"0","    PY >= 2010 &"
"0","    ("
"0","      str_detect(AB, regex(""learn.* disab.*"", ignore_case = TRUE)) |"
"0","      str_detect(DE, regex(""learn.* disab.*"", ignore_case = TRUE)) |"
"0","      str_detect(TI, regex(""learn.* disab.*"", ignore_case = TRUE))"
"0","    )"
"0","  )"
"0",""
"0","united_tbl_2010_later <- TextAnalysisR::unite_text_cols(df_2010_later, listed_vars = c(""AB"", ""DE"", ""TI""))"
"0",""
"0","tokens_2010_later <- TextAnalysisR::preprocess_texts(united_tbl_2010_later,"
"0","                                          text_field = ""united_texts"","
"0","                                          min_char = 2,"
"0","                                          remove_punct = TRUE,"
"0","                                          remove_symbols = TRUE,"
"0","                                          remove_numbers = TRUE,"
"0","                                          remove_url = TRUE,"
"0","                                          remove_separators = TRUE,"
"0","                                          split_hyphens = TRUE,"
"0","                                          split_tags = TRUE,"
"0","                                          include_docvars = TRUE,"
"0","                                          keep_acronyms = FALSE,"
"0","                                          padding = FALSE,"
"0","                                          verbose = FALSE)"
"2","G2;H2;Warningh: keep_acronyms argument is not used.g
"
"0","custom_dict <- quanteda::dictionary(list(custom = c(""learning disabilities"", ""single-case"", ""single-subject"", ""functional relation"", ""visual analysis"")))"
"0",""
"0","toks_compound_2010_later <- quanteda::tokens_compound("
"0","  tokens_2010_later,"
"0","  pattern = custom_dict,"
"0","  concatenator = ""_"","
"0","  valuetype = ""glob"","
"0","  window = 0,"
"0","  case_insensitive = TRUE,"
"0","  join = TRUE,"
"0","  keep_unigrams = FALSE,"
"0","  verbose = TRUE"
"0",")"
"2","G3;tokens_compound() changed from 29,088 tokens (148 documents) to 28,975 tokens (148 documents)
g"
"0","dfm_object_init_2010_later <- quanteda::dfm(toks_compound_2010_later)"
"0",""
"0","stopwords <- stopwords::stopwords(""en"", source = ""snowball"")"
"0",""
"0","toks_removed_2010_later <- quanteda::tokens_remove(toks_compound_2010_later, pattern = stopwords, verbose = FALSE)"
"0",""
"0","dfm_init_2010_later <- quanteda::dfm(toks_removed_2010_later)"
"0",""
"0","common_words <- c(""study"", ""students"", ""research"", ""results"")"
"0",""
"0","toks_removed_common_2010_later <- quanteda::tokens_remove(toks_removed_2010_later, pattern = common_words, verbose = FALSE)"
"0",""
"0","dfm_2010_later <- quanteda::dfm(toks_removed_common_2010_later)"
"0",""
"0","# TextAnalysisR::plot_word_frequency(dfm_2010_later, n = 20)"
"0",""
"0","word_network_2010_later <- word_correlation_network("
"0","  dfm_2010_later,"
"0","  doc_var = NULL,"
"0","  common_term_n = 3,"
"0","  corr_n = 0.27,"
"0","  top_node_n = 25,"
"0","  node_label = 30,"
"0","  nrows = 1,"
"0","  height = 1000,"
"0","  width = 1500,"
"0","  pattern = ""learn.*disab.*|single.*case|single.*subject|visual.*analy*|multi.*level|funtion.*relation|bayesian"","
"0","  showlegend = FALSE,"
"0","  seed = 2025"
"0",")"
"0",""
"0","network_2010_later_plot <- word_network_2010_later$plot  "
"0",""
"0","htmlwidgets::saveWidget(network_2010_later_plot, file = ""figures/network_2010_later_plot.html"", selfcontained = TRUE)"
